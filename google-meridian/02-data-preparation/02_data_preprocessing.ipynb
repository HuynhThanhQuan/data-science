{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Media Mix Modeling\n",
    "\n",
    "This notebook covers essential data preprocessing steps for MMM including:\n",
    "- Data cleaning and validation\n",
    "- Feature engineering\n",
    "- Handling seasonality\n",
    "- External factor integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load sample data (you can replace with your own dataset)\n",
    "# For now, we'll create synthetic data similar to the previous notebook\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='W')\n",
    "n_weeks = len(dates)\n",
    "\n",
    "print(f\"Working with {n_weeks} weeks of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Structure and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more realistic synthetic data with trends and seasonality\n",
    "def create_mmm_dataset(n_weeks, start_date='2020-01-01'):\n",
    "    dates = pd.date_range(start=start_date, periods=n_weeks, freq='W')\n",
    "    \n",
    "    # Base trend\n",
    "    trend = np.linspace(1, 1.5, n_weeks)\n",
    "    \n",
    "    # Seasonality (Christmas, summer peaks)\n",
    "    week_of_year = pd.Series(dates).dt.isocalendar().week\n",
    "    seasonality = 1 + 0.3 * np.sin(2 * np.pi * week_of_year / 52) + 0.2 * np.sin(4 * np.pi * week_of_year / 52)\n",
    "    \n",
    "    # Marketing channels with different patterns\n",
    "    tv_base = 8000 * trend * seasonality + np.random.normal(0, 1000, n_weeks)\n",
    "    digital_base = 5000 * trend * (1 + 0.1 * np.sin(2 * np.pi * week_of_year / 52)) + np.random.normal(0, 800, n_weeks)\n",
    "    radio_base = 3000 * trend + np.random.normal(0, 500, n_weeks)\n",
    "    print_base = 2000 * trend * (1 - 0.2 * np.sin(2 * np.pi * week_of_year / 52)) + np.random.normal(0, 400, n_weeks)\n",
    "    \n",
    "    # Ensure non-negative spending\n",
    "    tv_spend = np.maximum(tv_base, 0)\n",
    "    digital_spend = np.maximum(digital_base, 0)\n",
    "    radio_spend = np.maximum(radio_base, 0)\n",
    "    print_spend = np.maximum(print_base, 0)\n",
    "    \n",
    "    # Conversions influenced by spend (with adstock effects)\n",
    "    total_spend = tv_spend + digital_spend + radio_spend + print_spend\n",
    "    base_conversions = 1000 * trend\n",
    "    media_effect = 0.3 * tv_spend**0.7 + 0.4 * digital_spend**0.8 + 0.2 * radio_spend**0.6 + 0.1 * print_spend**0.5\n",
    "    conversions = base_conversions + media_effect * seasonality + np.random.normal(0, 200, n_weeks)\n",
    "    conversions = np.maximum(conversions, 0)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'tv_spend': tv_spend,\n",
    "        'digital_spend': digital_spend,\n",
    "        'radio_spend': radio_spend,\n",
    "        'print_spend': print_spend,\n",
    "        'conversions': conversions,\n",
    "        'week_of_year': week_of_year,\n",
    "        'trend': trend,\n",
    "        'seasonality': seasonality\n",
    "    })\n",
    "\n",
    "df = create_mmm_dataset(n_weeks)\n",
    "print(\"Dataset created successfully!\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time-based features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['is_holiday_season'] = ((df['month'] == 11) | (df['month'] == 12)).astype(int)\n",
    "\n",
    "# Add lagged variables (adstock effect simulation)\n",
    "def create_adstock_features(df, channels, max_lag=4, decay_rate=0.7):\n",
    "    for channel in channels:\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            col_name = f'{channel}_lag_{lag}'\n",
    "            df[col_name] = df[channel].shift(lag) * (decay_rate ** lag)\n",
    "    return df\n",
    "\n",
    "channels = ['tv_spend', 'digital_spend', 'radio_spend', 'print_spend']\n",
    "df = create_adstock_features(df, channels)\n",
    "\n",
    "# Create total spend metrics\n",
    "df['total_spend'] = df[channels].sum(axis=1)\n",
    "df['spend_per_conversion'] = df['total_spend'] / df['conversions']\n",
    "\n",
    "print(f\"Features added. Dataset now has {df.shape[1]} columns\")\n",
    "print(\"New feature columns:\")\n",
    "print([col for col in df.columns if 'lag' in col or col in ['total_spend', 'spend_per_conversion', 'is_holiday_season']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values and Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Handle missing values (forward fill for lagged features)\n",
    "lag_columns = [col for col in df.columns if 'lag' in col]\n",
    "df[lag_columns] = df[lag_columns].fillna(0)\n",
    "\n",
    "# Outlier detection using IQR method\n",
    "def detect_outliers_iqr(df, columns):\n",
    "    outliers = {}\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers[col] = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]\n",
    "    return outliers\n",
    "\n",
    "outliers = detect_outliers_iqr(df, channels + ['conversions'])\n",
    "print(\"\\nOutliers detected:\")\n",
    "for col, count in outliers.items():\n",
    "    print(f\"{col}: {count} outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Seasonality and Trend Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize seasonality patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Seasonal Patterns in Marketing Data', fontsize=16)\n",
    "\n",
    "# Monthly spend patterns\n",
    "monthly_spend = df.groupby('month')[channels].mean()\n",
    "axes[0, 0].plot(monthly_spend.index, monthly_spend.values)\n",
    "axes[0, 0].set_title('Average Monthly Spend by Channel')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Average Spend')\n",
    "axes[0, 0].legend(channels)\n",
    "\n",
    "# Quarterly conversions\n",
    "quarterly_conv = df.groupby('quarter')['conversions'].mean()\n",
    "axes[0, 1].bar(quarterly_conv.index, quarterly_conv.values)\n",
    "axes[0, 1].set_title('Average Quarterly Conversions')\n",
    "axes[0, 1].set_xlabel('Quarter')\n",
    "axes[0, 1].set_ylabel('Average Conversions')\n",
    "\n",
    "# Year-over-year trends\n",
    "yearly_totals = df.groupby('year')[['total_spend', 'conversions']].sum()\n",
    "ax1 = axes[1, 0]\n",
    "ax2 = ax1.twinx()\n",
    "ax1.bar(yearly_totals.index, yearly_totals['total_spend'], alpha=0.7, color='blue')\n",
    "ax2.plot(yearly_totals.index, yearly_totals['conversions'], color='red', marker='o')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Total Spend', color='blue')\n",
    "ax2.set_ylabel('Total Conversions', color='red')\n",
    "axes[1, 0].set_title('Year-over-Year Trends')\n",
    "\n",
    "# Efficiency over time\n",
    "monthly_efficiency = df.groupby(['year', 'month'])['spend_per_conversion'].mean().reset_index()\n",
    "monthly_efficiency['date'] = pd.to_datetime(monthly_efficiency[['year', 'month']].assign(day=1))\n",
    "axes[1, 1].plot(monthly_efficiency['date'], monthly_efficiency['spend_per_conversion'])\n",
    "axes[1, 1].set_title('Spend per Conversion Over Time')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('Spend per Conversion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "feature_columns = channels + lag_columns + ['total_spend', 'is_holiday_season']\n",
    "feature_columns = [col for col in feature_columns if col in df.columns]\n",
    "\n",
    "# Remove any remaining NaN values\n",
    "df_clean = df.dropna(subset=feature_columns + ['conversions'])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df_clean.copy()\n",
    "df_scaled[feature_columns] = scaler.fit_transform(df_clean[feature_columns])\n",
    "\n",
    "print(f\"Data prepared for modeling:\")\n",
    "print(f\"Shape: {df_scaled.shape}\")\n",
    "print(f\"Features: {len(feature_columns)}\")\n",
    "print(f\"Target variable: conversions\")\n",
    "\n",
    "# Save processed data\n",
    "df_scaled.to_csv('../data/processed_mmm_data.csv', index=False)\n",
    "print(\"\\nProcessed data saved to ../data/processed_mmm_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Data Quality Assessment\n",
    "\n",
    "Complete the following data quality checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete these data quality checks\n",
    "\n",
    "# 1. Check for data consistency (e.g., no negative spending)\n",
    "# Your code here\n",
    "\n",
    "# 2. Validate date ranges and frequency\n",
    "# Your code here\n",
    "\n",
    "# 3. Check correlation between channels (multicollinearity)\n",
    "# Your code here\n",
    "\n",
    "# 4. Validate business logic (e.g., reasonable spend-to-conversion ratios)\n",
    "# Your code here\n",
    "\n",
    "print(\"Complete the data quality assessment above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next module (`03-modeling/`), we'll:\n",
    "- Build basic MMM models\n",
    "- Implement adstock and saturation curves\n",
    "- Use Bayesian approaches for uncertainty quantification\n",
    "\n",
    "Your data is now ready for modeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}